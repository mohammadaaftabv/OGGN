{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#imports necessary to define a neural network \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#ensure you are using GPU.\n",
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\"  \n",
    "\n",
    "device = torch.device(dev)\n",
    "print(device)\n",
    "\n",
    "dtype = torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0=9.0\n",
    "a1=8.97\n",
    "a2=0.876\n",
    "a3=2.9876\n",
    "\n",
    "def output(x0,x1,x2,x3):\n",
    "    \n",
    "    out=a0*x0**(0.87)\n",
    "    out=out+a1*x1**(0.020)\n",
    "    out=out+a2*x2**(0.12)\n",
    "    out=out+a3*x3**(0.987)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain=1000.0*torch.rand(10000,4).type(dtype)\n",
    "xtest=1000.0*torch.rand(1000,4).type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest=torch.empty(1000,1).type(dtype)\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    ytest[i]=output(xtest[i][0],xtest[i][1],xtest[i][2],xtest[i][3]).type(dtype)\n",
    "\n",
    "ytrain=torch.empty(10000,1).type(dtype)\n",
    "\n",
    "for i in range(10000):\n",
    "    \n",
    "    ytrain[i]=output(xtrain[i][0],xtrain[i][1],xtrain[i][2],xtrain[i][3]).type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Forward(nn.Module):\n",
    "    def __init__(self):\n",
    "    \n",
    "        super(Forward, self).__init__()\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(4,80)\n",
    "        self.fc2 = nn.Linear(80,160)\n",
    "        self.fc4=nn.Linear(160,16)\n",
    "        self.fc3=nn.Linear(16,1)\n",
    "        '''\n",
    "        \n",
    "        self.fc3 = nn.Linear(6400, 12800)\n",
    "\n",
    "        self.fc5=nn.Linear(12800, 12800)\n",
    "        self.fc6=nn.Linear(12800, 1280)\n",
    "        self.fc4 = nn.Linear(1280, 784)\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        '''\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.fc1(x)) # (input, negative_slope=0.2)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = F.relu(self.fc4(x))\n",
    "        \n",
    "        x = F.relu(self.fc3(x))\n",
    "        \n",
    "        '''\n",
    "        out =F.relu(self.fc5(x))\n",
    "        out =F.relu(self.fc6(out))\n",
    "        out =F.relu(self.fc4(out))\n",
    "        out[out>255]=out[out>255]/100\n",
    "        out[out<55]=out[out<55]*1000\n",
    "        '''\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward(\n",
      "  (fc1): Linear(in_features=4, out_features=80, bias=True)\n",
      "  (fc2): Linear(in_features=80, out_features=160, bias=True)\n",
      "  (fc4): Linear(in_features=160, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "learning_rate=0.001\n",
    "momentum=0.05\n",
    "\n",
    "forwardnetwork = Forward().cuda()\n",
    "forwardoptimizer = optim.Adagrad(forwardnetwork.parameters(), lr=learning_rate,\n",
    "                      )\n",
    "print(forwardnetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]], device='cuda:0', grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forwardnetwork(xtrain[0:10].reshape(10,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.6701e+09, device='cuda:0', grad_fn=<AddBackward0>) tensor(386127.5312, device='cuda:0', grad_fn=<MseLossBackward>) 0\n",
      "tensor(1.1923e+08, device='cuda:0', grad_fn=<AddBackward0>) tensor(11459.0557, device='cuda:0', grad_fn=<MseLossBackward>) 1\n",
      "tensor(10262858., device='cuda:0', grad_fn=<AddBackward0>) tensor(6507.4673, device='cuda:0', grad_fn=<MseLossBackward>) 2\n",
      "tensor(7242262.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(5416.7471, device='cuda:0', grad_fn=<MseLossBackward>) 3\n",
      "tensor(6309479.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(4912.3633, device='cuda:0', grad_fn=<MseLossBackward>) 4\n",
      "tensor(5800711., device='cuda:0', grad_fn=<AddBackward0>) tensor(4591.8550, device='cuda:0', grad_fn=<MseLossBackward>) 5\n",
      "tensor(5455793., device='cuda:0', grad_fn=<AddBackward0>) tensor(4358.3506, device='cuda:0', grad_fn=<MseLossBackward>) 6\n",
      "tensor(5202983., device='cuda:0', grad_fn=<AddBackward0>) tensor(4176.3701, device='cuda:0', grad_fn=<MseLossBackward>) 7\n",
      "tensor(4998316., device='cuda:0', grad_fn=<AddBackward0>) tensor(4032.7642, device='cuda:0', grad_fn=<MseLossBackward>) 8\n",
      "tensor(4838499.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3915.8252, device='cuda:0', grad_fn=<MseLossBackward>) 9\n",
      "tensor(4710890.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3824.9646, device='cuda:0', grad_fn=<MseLossBackward>) 10\n",
      "tensor(4608228., device='cuda:0', grad_fn=<AddBackward0>) tensor(3750.7566, device='cuda:0', grad_fn=<MseLossBackward>) 11\n",
      "tensor(4523148., device='cuda:0', grad_fn=<AddBackward0>) tensor(3687.0249, device='cuda:0', grad_fn=<MseLossBackward>) 12\n",
      "tensor(4451467., device='cuda:0', grad_fn=<AddBackward0>) tensor(3633.0396, device='cuda:0', grad_fn=<MseLossBackward>) 13\n",
      "tensor(4390737.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3587.3184, device='cuda:0', grad_fn=<MseLossBackward>) 14\n",
      "tensor(4339144.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3546.8494, device='cuda:0', grad_fn=<MseLossBackward>) 15\n",
      "tensor(4293703.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3510.8896, device='cuda:0', grad_fn=<MseLossBackward>) 16\n",
      "tensor(4244972., device='cuda:0', grad_fn=<AddBackward0>) tensor(3471.6741, device='cuda:0', grad_fn=<MseLossBackward>) 17\n",
      "tensor(4206002., device='cuda:0', grad_fn=<AddBackward0>) tensor(3443.4597, device='cuda:0', grad_fn=<MseLossBackward>) 18\n",
      "tensor(4173485.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3418.2212, device='cuda:0', grad_fn=<MseLossBackward>) 19\n",
      "tensor(4144446.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3394.3286, device='cuda:0', grad_fn=<MseLossBackward>) 20\n",
      "tensor(4118333.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3374.5552, device='cuda:0', grad_fn=<MseLossBackward>) 21\n",
      "tensor(4095470.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3356.5583, device='cuda:0', grad_fn=<MseLossBackward>) 22\n",
      "tensor(4074732., device='cuda:0', grad_fn=<AddBackward0>) tensor(3340.2244, device='cuda:0', grad_fn=<MseLossBackward>) 23\n",
      "tensor(4055733., device='cuda:0', grad_fn=<AddBackward0>) tensor(3325.4021, device='cuda:0', grad_fn=<MseLossBackward>) 24\n",
      "tensor(4037872., device='cuda:0', grad_fn=<AddBackward0>) tensor(3311.6602, device='cuda:0', grad_fn=<MseLossBackward>) 25\n",
      "tensor(4021617.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3299.2537, device='cuda:0', grad_fn=<MseLossBackward>) 26\n",
      "tensor(4006477., device='cuda:0', grad_fn=<AddBackward0>) tensor(3287.7136, device='cuda:0', grad_fn=<MseLossBackward>) 27\n",
      "tensor(3992380., device='cuda:0', grad_fn=<AddBackward0>) tensor(3276.8699, device='cuda:0', grad_fn=<MseLossBackward>) 28\n",
      "tensor(3979037., device='cuda:0', grad_fn=<AddBackward0>) tensor(3266.7874, device='cuda:0', grad_fn=<MseLossBackward>) 29\n",
      "tensor(3966893., device='cuda:0', grad_fn=<AddBackward0>) tensor(3258.0413, device='cuda:0', grad_fn=<MseLossBackward>) 30\n",
      "tensor(3955500.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3250.1631, device='cuda:0', grad_fn=<MseLossBackward>) 31\n",
      "tensor(3944714.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3243.1772, device='cuda:0', grad_fn=<MseLossBackward>) 32\n",
      "tensor(3934710., device='cuda:0', grad_fn=<AddBackward0>) tensor(3236.4641, device='cuda:0', grad_fn=<MseLossBackward>) 33\n",
      "tensor(3925232.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3230.2712, device='cuda:0', grad_fn=<MseLossBackward>) 34\n",
      "tensor(3916059.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3224.3904, device='cuda:0', grad_fn=<MseLossBackward>) 35\n",
      "tensor(3907448.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3218.7678, device='cuda:0', grad_fn=<MseLossBackward>) 36\n",
      "tensor(3899207.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3213.1204, device='cuda:0', grad_fn=<MseLossBackward>) 37\n",
      "tensor(3891117., device='cuda:0', grad_fn=<AddBackward0>) tensor(3207.3594, device='cuda:0', grad_fn=<MseLossBackward>) 38\n",
      "tensor(3883602.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3202.1169, device='cuda:0', grad_fn=<MseLossBackward>) 39\n",
      "tensor(3876589.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3197.0852, device='cuda:0', grad_fn=<MseLossBackward>) 40\n",
      "tensor(3869793.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3192.1787, device='cuda:0', grad_fn=<MseLossBackward>) 41\n",
      "tensor(3863245.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3187.1763, device='cuda:0', grad_fn=<MseLossBackward>) 42\n",
      "tensor(3856964.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3182.5142, device='cuda:0', grad_fn=<MseLossBackward>) 43\n",
      "tensor(3850887.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3177.7971, device='cuda:0', grad_fn=<MseLossBackward>) 44\n",
      "tensor(3845081.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3173.6331, device='cuda:0', grad_fn=<MseLossBackward>) 45\n",
      "tensor(3839464.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3168.7480, device='cuda:0', grad_fn=<MseLossBackward>) 46\n",
      "tensor(3833849., device='cuda:0', grad_fn=<AddBackward0>) tensor(3164.4692, device='cuda:0', grad_fn=<MseLossBackward>) 47\n",
      "tensor(3828418.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3160.4795, device='cuda:0', grad_fn=<MseLossBackward>) 48\n",
      "tensor(3823223.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3156.6147, device='cuda:0', grad_fn=<MseLossBackward>) 49\n",
      "tensor(3818280.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3153.1267, device='cuda:0', grad_fn=<MseLossBackward>) 50\n",
      "tensor(3813479.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3149.5176, device='cuda:0', grad_fn=<MseLossBackward>) 51\n",
      "tensor(3808852.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3145.7612, device='cuda:0', grad_fn=<MseLossBackward>) 52\n",
      "tensor(3804246.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3142.3169, device='cuda:0', grad_fn=<MseLossBackward>) 53\n",
      "tensor(3799860., device='cuda:0', grad_fn=<AddBackward0>) tensor(3139.0032, device='cuda:0', grad_fn=<MseLossBackward>) 54\n",
      "tensor(3795542., device='cuda:0', grad_fn=<AddBackward0>) tensor(3135.8018, device='cuda:0', grad_fn=<MseLossBackward>) 55\n",
      "tensor(3791322.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3132.2822, device='cuda:0', grad_fn=<MseLossBackward>) 56\n",
      "tensor(3787318.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3128.7930, device='cuda:0', grad_fn=<MseLossBackward>) 57\n",
      "tensor(3783246.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3125.3281, device='cuda:0', grad_fn=<MseLossBackward>) 58\n",
      "tensor(3779420., device='cuda:0', grad_fn=<AddBackward0>) tensor(3122.0417, device='cuda:0', grad_fn=<MseLossBackward>) 59\n",
      "tensor(3775634.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3118.8982, device='cuda:0', grad_fn=<MseLossBackward>) 60\n",
      "tensor(3771821.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3115.6697, device='cuda:0', grad_fn=<MseLossBackward>) 61\n",
      "tensor(3768244., device='cuda:0', grad_fn=<AddBackward0>) tensor(3112.5166, device='cuda:0', grad_fn=<MseLossBackward>) 62\n",
      "tensor(3764653.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3109.4026, device='cuda:0', grad_fn=<MseLossBackward>) 63\n",
      "tensor(3761265.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3106.6487, device='cuda:0', grad_fn=<MseLossBackward>) 64\n",
      "tensor(3757898., device='cuda:0', grad_fn=<AddBackward0>) tensor(3103.7969, device='cuda:0', grad_fn=<MseLossBackward>) 65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3754691., device='cuda:0', grad_fn=<AddBackward0>) tensor(3101.0420, device='cuda:0', grad_fn=<MseLossBackward>) 66\n",
      "tensor(3751547.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3098.4661, device='cuda:0', grad_fn=<MseLossBackward>) 67\n",
      "tensor(3748509., device='cuda:0', grad_fn=<AddBackward0>) tensor(3095.8335, device='cuda:0', grad_fn=<MseLossBackward>) 68\n",
      "tensor(3745472.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3093.2122, device='cuda:0', grad_fn=<MseLossBackward>) 69\n",
      "tensor(3742538., device='cuda:0', grad_fn=<AddBackward0>) tensor(3090.7732, device='cuda:0', grad_fn=<MseLossBackward>) 70\n",
      "tensor(3739646.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3088.2026, device='cuda:0', grad_fn=<MseLossBackward>) 71\n",
      "tensor(3736702.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3085.7654, device='cuda:0', grad_fn=<MseLossBackward>) 72\n",
      "tensor(3733924., device='cuda:0', grad_fn=<AddBackward0>) tensor(3083.4321, device='cuda:0', grad_fn=<MseLossBackward>) 73\n",
      "tensor(3731167.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3081.1807, device='cuda:0', grad_fn=<MseLossBackward>) 74\n",
      "tensor(3728427.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3079.0496, device='cuda:0', grad_fn=<MseLossBackward>) 75\n",
      "tensor(3725831.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3076.8613, device='cuda:0', grad_fn=<MseLossBackward>) 76\n",
      "tensor(3723242.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3074.7175, device='cuda:0', grad_fn=<MseLossBackward>) 77\n",
      "tensor(3720658.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3072.6055, device='cuda:0', grad_fn=<MseLossBackward>) 78\n",
      "tensor(3718114.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3070.5386, device='cuda:0', grad_fn=<MseLossBackward>) 79\n",
      "tensor(3715553., device='cuda:0', grad_fn=<AddBackward0>) tensor(3068.4792, device='cuda:0', grad_fn=<MseLossBackward>) 80\n",
      "tensor(3713110.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3066.5195, device='cuda:0', grad_fn=<MseLossBackward>) 81\n",
      "tensor(3710713.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3064.5505, device='cuda:0', grad_fn=<MseLossBackward>) 82\n",
      "tensor(3708368.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3062.6965, device='cuda:0', grad_fn=<MseLossBackward>) 83\n",
      "tensor(3706028.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3060.7808, device='cuda:0', grad_fn=<MseLossBackward>) 84\n",
      "tensor(3703717.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3058.8486, device='cuda:0', grad_fn=<MseLossBackward>) 85\n",
      "tensor(3701513.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3056.9492, device='cuda:0', grad_fn=<MseLossBackward>) 86\n",
      "tensor(3699241.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3055.0222, device='cuda:0', grad_fn=<MseLossBackward>) 87\n",
      "tensor(3697039.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3053.1416, device='cuda:0', grad_fn=<MseLossBackward>) 88\n",
      "tensor(3694881., device='cuda:0', grad_fn=<AddBackward0>) tensor(3051.3015, device='cuda:0', grad_fn=<MseLossBackward>) 89\n",
      "tensor(3692793.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3049.5842, device='cuda:0', grad_fn=<MseLossBackward>) 90\n",
      "tensor(3690665.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3047.8652, device='cuda:0', grad_fn=<MseLossBackward>) 91\n",
      "tensor(3688614.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3046.1028, device='cuda:0', grad_fn=<MseLossBackward>) 92\n",
      "tensor(3686562.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3044.3931, device='cuda:0', grad_fn=<MseLossBackward>) 93\n",
      "tensor(3684538.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3042.7036, device='cuda:0', grad_fn=<MseLossBackward>) 94\n",
      "tensor(3682609., device='cuda:0', grad_fn=<AddBackward0>) tensor(3040.9817, device='cuda:0', grad_fn=<MseLossBackward>) 95\n",
      "tensor(3680574., device='cuda:0', grad_fn=<AddBackward0>) tensor(3039.1982, device='cuda:0', grad_fn=<MseLossBackward>) 96\n",
      "tensor(3678560., device='cuda:0', grad_fn=<AddBackward0>) tensor(3037.5291, device='cuda:0', grad_fn=<MseLossBackward>) 97\n",
      "tensor(3676571.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3035.8694, device='cuda:0', grad_fn=<MseLossBackward>) 98\n",
      "tensor(3674697.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3034.1643, device='cuda:0', grad_fn=<MseLossBackward>) 99\n",
      "tensor(3672752.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3032.5322, device='cuda:0', grad_fn=<MseLossBackward>) 100\n",
      "tensor(3670856.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3030.9666, device='cuda:0', grad_fn=<MseLossBackward>) 101\n",
      "tensor(3668888.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3029.3625, device='cuda:0', grad_fn=<MseLossBackward>) 102\n",
      "tensor(3667106.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3027.8174, device='cuda:0', grad_fn=<MseLossBackward>) 103\n",
      "tensor(3665231.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3026.3237, device='cuda:0', grad_fn=<MseLossBackward>) 104\n",
      "tensor(3663495.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3024.8372, device='cuda:0', grad_fn=<MseLossBackward>) 105\n",
      "tensor(3661680.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3023.3706, device='cuda:0', grad_fn=<MseLossBackward>) 106\n",
      "tensor(3659889.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3021.8589, device='cuda:0', grad_fn=<MseLossBackward>) 107\n",
      "tensor(3658113.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3020.3721, device='cuda:0', grad_fn=<MseLossBackward>) 108\n",
      "tensor(3656298.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3018.8762, device='cuda:0', grad_fn=<MseLossBackward>) 109\n",
      "tensor(3654547.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3017.3777, device='cuda:0', grad_fn=<MseLossBackward>) 110\n",
      "tensor(3652792.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3015.9233, device='cuda:0', grad_fn=<MseLossBackward>) 111\n",
      "tensor(3651072.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3014.4651, device='cuda:0', grad_fn=<MseLossBackward>) 112\n",
      "tensor(3649360.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3013.0264, device='cuda:0', grad_fn=<MseLossBackward>) 113\n",
      "tensor(3647694., device='cuda:0', grad_fn=<AddBackward0>) tensor(3011.5864, device='cuda:0', grad_fn=<MseLossBackward>) 114\n",
      "tensor(3645948.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3010.1592, device='cuda:0', grad_fn=<MseLossBackward>) 115\n",
      "tensor(3644268., device='cuda:0', grad_fn=<AddBackward0>) tensor(3008.7617, device='cuda:0', grad_fn=<MseLossBackward>) 116\n",
      "tensor(3642608.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3007.3760, device='cuda:0', grad_fn=<MseLossBackward>) 117\n",
      "tensor(3640968.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(3005.9446, device='cuda:0', grad_fn=<MseLossBackward>) 118\n",
      "tensor(3639352., device='cuda:0', grad_fn=<AddBackward0>) tensor(3004.5720, device='cuda:0', grad_fn=<MseLossBackward>) 119\n",
      "tensor(3637764., device='cuda:0', grad_fn=<AddBackward0>) tensor(3003.2446, device='cuda:0', grad_fn=<MseLossBackward>) 120\n",
      "tensor(3636199.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(3001.9565, device='cuda:0', grad_fn=<MseLossBackward>) 121\n",
      "tensor(3634639., device='cuda:0', grad_fn=<AddBackward0>) tensor(3000.5522, device='cuda:0', grad_fn=<MseLossBackward>) 122\n",
      "tensor(3633098.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2999.2581, device='cuda:0', grad_fn=<MseLossBackward>) 123\n",
      "tensor(3631591.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2997.9326, device='cuda:0', grad_fn=<MseLossBackward>) 124\n",
      "tensor(3630084.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2996.6680, device='cuda:0', grad_fn=<MseLossBackward>) 125\n",
      "tensor(3628588.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(2995.4233, device='cuda:0', grad_fn=<MseLossBackward>) 126\n",
      "tensor(3627106., device='cuda:0', grad_fn=<AddBackward0>) tensor(2994.2102, device='cuda:0', grad_fn=<MseLossBackward>) 127\n",
      "tensor(3625646.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2992.9963, device='cuda:0', grad_fn=<MseLossBackward>) 128\n",
      "tensor(3624193.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2991.7322, device='cuda:0', grad_fn=<MseLossBackward>) 129\n",
      "tensor(3622719.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2990.4382, device='cuda:0', grad_fn=<MseLossBackward>) 130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3621291.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2989.2227, device='cuda:0', grad_fn=<MseLossBackward>) 131\n",
      "tensor(3619883.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(2988.0415, device='cuda:0', grad_fn=<MseLossBackward>) 132\n",
      "tensor(3618500.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(2986.8550, device='cuda:0', grad_fn=<MseLossBackward>) 133\n",
      "tensor(3617108.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2985.6650, device='cuda:0', grad_fn=<MseLossBackward>) 134\n",
      "tensor(3615740.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2984.4922, device='cuda:0', grad_fn=<MseLossBackward>) 135\n",
      "tensor(3614314.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2983.2756, device='cuda:0', grad_fn=<MseLossBackward>) 136\n",
      "tensor(3613006.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2982.1042, device='cuda:0', grad_fn=<MseLossBackward>) 137\n",
      "tensor(3611637., device='cuda:0', grad_fn=<AddBackward0>) tensor(2980.9712, device='cuda:0', grad_fn=<MseLossBackward>) 138\n",
      "tensor(3610299.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(2979.8811, device='cuda:0', grad_fn=<MseLossBackward>) 139\n",
      "tensor(3608990.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(2978.7615, device='cuda:0', grad_fn=<MseLossBackward>) 140\n",
      "tensor(3607653.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2977.6658, device='cuda:0', grad_fn=<MseLossBackward>) 141\n",
      "tensor(3606329., device='cuda:0', grad_fn=<AddBackward0>) tensor(2976.6050, device='cuda:0', grad_fn=<MseLossBackward>) 142\n",
      "tensor(3605024.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2975.5610, device='cuda:0', grad_fn=<MseLossBackward>) 143\n",
      "tensor(3603722., device='cuda:0', grad_fn=<AddBackward0>) tensor(2974.5112, device='cuda:0', grad_fn=<MseLossBackward>) 144\n",
      "tensor(3602453., device='cuda:0', grad_fn=<AddBackward0>) tensor(2973.5173, device='cuda:0', grad_fn=<MseLossBackward>) 145\n",
      "tensor(3601180.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2972.4941, device='cuda:0', grad_fn=<MseLossBackward>) 146\n",
      "tensor(3599902., device='cuda:0', grad_fn=<AddBackward0>) tensor(2971.4768, device='cuda:0', grad_fn=<MseLossBackward>) 147\n",
      "tensor(3598637.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(2970.4688, device='cuda:0', grad_fn=<MseLossBackward>) 148\n",
      "tensor(3597368., device='cuda:0', grad_fn=<AddBackward0>) tensor(2969.4702, device='cuda:0', grad_fn=<MseLossBackward>) 149\n",
      "tensor(3596078., device='cuda:0', grad_fn=<AddBackward0>) tensor(2968.4583, device='cuda:0', grad_fn=<MseLossBackward>) 150\n",
      "tensor(3594817.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2967.4580, device='cuda:0', grad_fn=<MseLossBackward>) 151\n",
      "tensor(3593616.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2966.4966, device='cuda:0', grad_fn=<MseLossBackward>) 152\n",
      "tensor(3592372.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2965.5393, device='cuda:0', grad_fn=<MseLossBackward>) 153\n",
      "tensor(3591141.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(2964.5820, device='cuda:0', grad_fn=<MseLossBackward>) 154\n",
      "tensor(3589917., device='cuda:0', grad_fn=<AddBackward0>) tensor(2963.6477, device='cuda:0', grad_fn=<MseLossBackward>) 155\n",
      "tensor(3588696.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(2962.7517, device='cuda:0', grad_fn=<MseLossBackward>) 156\n",
      "tensor(3587496.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(2961.8462, device='cuda:0', grad_fn=<MseLossBackward>) 157\n",
      "tensor(3586319.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2960.9700, device='cuda:0', grad_fn=<MseLossBackward>) 158\n",
      "tensor(3585137., device='cuda:0', grad_fn=<AddBackward0>) tensor(2959.9956, device='cuda:0', grad_fn=<MseLossBackward>) 159\n",
      "tensor(3583936.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(2959.0710, device='cuda:0', grad_fn=<MseLossBackward>) 160\n",
      "tensor(3582770.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2958.1245, device='cuda:0', grad_fn=<MseLossBackward>) 161\n",
      "tensor(3581599., device='cuda:0', grad_fn=<AddBackward0>) tensor(2957.2222, device='cuda:0', grad_fn=<MseLossBackward>) 162\n",
      "tensor(3580442., device='cuda:0', grad_fn=<AddBackward0>) tensor(2956.2915, device='cuda:0', grad_fn=<MseLossBackward>) 163\n",
      "tensor(3579306.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2955.3613, device='cuda:0', grad_fn=<MseLossBackward>) 164\n",
      "tensor(3578119.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2954.4561, device='cuda:0', grad_fn=<MseLossBackward>) 165\n",
      "tensor(3576973.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2953.5559, device='cuda:0', grad_fn=<MseLossBackward>) 166\n",
      "tensor(3575813.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2952.6672, device='cuda:0', grad_fn=<MseLossBackward>) 167\n",
      "tensor(3574685., device='cuda:0', grad_fn=<AddBackward0>) tensor(2951.7546, device='cuda:0', grad_fn=<MseLossBackward>) 168\n",
      "tensor(3573540.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2950.8408, device='cuda:0', grad_fn=<MseLossBackward>) 169\n",
      "tensor(3572405., device='cuda:0', grad_fn=<AddBackward0>) tensor(2949.9170, device='cuda:0', grad_fn=<MseLossBackward>) 170\n",
      "tensor(3571241.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2949.0381, device='cuda:0', grad_fn=<MseLossBackward>) 171\n",
      "tensor(3570111.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2948.1792, device='cuda:0', grad_fn=<MseLossBackward>) 172\n",
      "tensor(3569036.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2947.3232, device='cuda:0', grad_fn=<MseLossBackward>) 173\n",
      "tensor(3567915., device='cuda:0', grad_fn=<AddBackward0>) tensor(2946.4614, device='cuda:0', grad_fn=<MseLossBackward>) 174\n",
      "tensor(3566785., device='cuda:0', grad_fn=<AddBackward0>) tensor(2945.5476, device='cuda:0', grad_fn=<MseLossBackward>) 175\n",
      "tensor(3565683.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2944.6934, device='cuda:0', grad_fn=<MseLossBackward>) 176\n",
      "tensor(3564611.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(2943.8706, device='cuda:0', grad_fn=<MseLossBackward>) 177\n",
      "tensor(3563519.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(2943.0142, device='cuda:0', grad_fn=<MseLossBackward>) 178\n",
      "tensor(3562415.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(2942.1301, device='cuda:0', grad_fn=<MseLossBackward>) 179\n",
      "tensor(3561334.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2941.2996, device='cuda:0', grad_fn=<MseLossBackward>) 180\n",
      "tensor(3560272., device='cuda:0', grad_fn=<AddBackward0>) tensor(2940.4414, device='cuda:0', grad_fn=<MseLossBackward>) 181\n",
      "tensor(3559181.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(2939.5962, device='cuda:0', grad_fn=<MseLossBackward>) 182\n",
      "tensor(3558091.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2938.7302, device='cuda:0', grad_fn=<MseLossBackward>) 183\n",
      "tensor(3557003., device='cuda:0', grad_fn=<AddBackward0>) tensor(2937.8799, device='cuda:0', grad_fn=<MseLossBackward>) 184\n",
      "tensor(3555960.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(2937.0686, device='cuda:0', grad_fn=<MseLossBackward>) 185\n",
      "tensor(3554886., device='cuda:0', grad_fn=<AddBackward0>) tensor(2936.1921, device='cuda:0', grad_fn=<MseLossBackward>) 186\n",
      "tensor(3553787.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2935.3357, device='cuda:0', grad_fn=<MseLossBackward>) 187\n",
      "tensor(3552734.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2934.5259, device='cuda:0', grad_fn=<MseLossBackward>) 188\n",
      "tensor(3551694.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2933.6816, device='cuda:0', grad_fn=<MseLossBackward>) 189\n",
      "tensor(3550659.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(2932.8596, device='cuda:0', grad_fn=<MseLossBackward>) 190\n",
      "tensor(3549626.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2932.0264, device='cuda:0', grad_fn=<MseLossBackward>) 191\n",
      "tensor(3548581.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2931.2458, device='cuda:0', grad_fn=<MseLossBackward>) 192\n",
      "tensor(3547582.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2930.4646, device='cuda:0', grad_fn=<MseLossBackward>) 193\n",
      "tensor(3546561., device='cuda:0', grad_fn=<AddBackward0>) tensor(2929.6982, device='cuda:0', grad_fn=<MseLossBackward>) 194\n",
      "tensor(3545541.2500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2928.8528, device='cuda:0', grad_fn=<MseLossBackward>) 195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3544534.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(2928.1067, device='cuda:0', grad_fn=<MseLossBackward>) 196\n",
      "tensor(3543516.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2927.3420, device='cuda:0', grad_fn=<MseLossBackward>) 197\n",
      "tensor(3542526.5000, device='cuda:0', grad_fn=<AddBackward0>) tensor(2926.6228, device='cuda:0', grad_fn=<MseLossBackward>) 198\n",
      "tensor(3541545., device='cuda:0', grad_fn=<AddBackward0>) tensor(2925.8301, device='cuda:0', grad_fn=<MseLossBackward>) 199\n",
      "tensor(3540564.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(2925.0610, device='cuda:0', grad_fn=<MseLossBackward>) 200\n",
      "tensor(3539585., device='cuda:0', grad_fn=<AddBackward0>) tensor(2924.2847, device='cuda:0', grad_fn=<MseLossBackward>) 201\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9a356c3470a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#clip gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwardnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mforwardoptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m   1291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m             \u001b[0m_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# noqa: C416 TODO: rewrite as list(range(m))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;31m# TODO: when https://github.com/pytorch/pytorch/issues/33782 is fixed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#training loop\n",
    "\n",
    "batchsize=8\n",
    "\n",
    "numbatches=int(len(xtrain)/batchsize)\n",
    "\n",
    "nepochs=1000\n",
    "criterion=nn.MSELoss()\n",
    "\n",
    "for i in range(nepochs):\n",
    "    \n",
    "    epochloss=0\n",
    "    \n",
    "    for j in range(numbatches):\n",
    "        \n",
    "        out=forwardnetwork(xtrain[j*batchsize:(j+1)*batchsize].reshape(batchsize,4)).type(dtype)\n",
    "        out=out.reshape(batchsize,1)\n",
    "        #print(out)\n",
    "        loss=criterion(out,ytrain[j*batchsize:(j+1)*batchsize].reshape(batchsize,1)).type(dtype)\n",
    "        \n",
    "        #backprop loss i.e. find dloss/dparam for each parameter and store.\n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        #clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(forwardnetwork.parameters(), 100.0)\n",
    "        forwardoptimizer.step()\n",
    "        \n",
    "        epochloss=epochloss+loss\n",
    "        \n",
    "        testout=forwardnetwork(xtest.reshape(1000,4))\n",
    "        lossc=nn.MSELoss()\n",
    "        testloss=lossc(testout,ytest)\n",
    "        \n",
    "    print(epochloss,testloss,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9883, 0.8938, 0.3498, 0.5575],\n",
      "        [0.7493, 0.6551, 0.6728, 0.4101],\n",
      "        [0.0183, 0.3512, 0.7787, 0.2225],\n",
      "        [0.6372, 0.1816, 0.0378, 0.1345],\n",
      "        [0.1076, 0.0731, 0.6858, 0.6667],\n",
      "        [0.3017, 0.9107, 0.1136, 0.4574],\n",
      "        [0.6593, 0.8011, 0.7639, 0.9189],\n",
      "        [0.9116, 0.7433, 0.8835, 0.6052],\n",
      "        [0.5369, 0.6302, 0.5495, 0.3568],\n",
      "        [0.6078, 0.9860, 0.6966, 0.9101]], device='cuda:0')\n",
      "tensor([[16.2022],\n",
      "        [14.8282],\n",
      "        [11.3781],\n",
      "        [13.4947],\n",
      "        [13.1002],\n",
      "        [13.3713],\n",
      "        [16.0859],\n",
      "        [16.0504],\n",
      "        [13.8775],\n",
      "        [15.8675]], device='cuda:0', grad_fn=<ReluBackward0>)\n",
      "tensor(20.3088, device='cuda:0')\n",
      "tensor(17.9708, device='cuda:0')\n",
      "tensor(10.5886, device='cuda:0')\n",
      "tensor(15.7538, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "xtes=torch.rand(10,4).type(dtype)\n",
    "ypred=forwardnetwork(xtes)\n",
    "yreal=output(xtes[0][0],xtes[0][1],xtes[0][2],xtes[0][3])\n",
    "\n",
    "print(xtes)\n",
    "print(ypred)\n",
    "print(yreal)\n",
    "print(output(xtes[1][0],xtes[1][1],xtes[1][2],xtes[1][3]))\n",
    "\n",
    "print(output(xtes[2][0],xtes[2][1],xtes[2][2],xtes[2][3]))\n",
    "\n",
    "print(output(xtes[3][0],xtes[3][1],xtes[3][2],xtes[3][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "    \n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(40,80)\n",
    "        self.fc2 = nn.Linear(80,16)\n",
    "        self.fc4=nn.Linear(16,16)\n",
    "        self.fc3=nn.Linear(16,3)\n",
    "        '''\n",
    "        \n",
    "        self.fc3 = nn.Linear(6400, 12800)\n",
    "\n",
    "        self.fc5=nn.Linear(12800, 12800)\n",
    "        self.fc6=nn.Linear(12800, 1280)\n",
    "        self.fc4 = nn.Linear(1280, 784)\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        '''\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.fc1(x)) # (input, negative_slope=0.2)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = F.relu(self.fc4(x))\n",
    "        \n",
    "        x = F.relu(self.fc3(x))\n",
    "        x[x>100]=x[x>100]/20\n",
    "        x[x<1]=x[x<1]*10\n",
    "        '''\n",
    "        out =F.relu(self.fc5(x))\n",
    "        out =F.relu(self.fc6(out))\n",
    "        out =F.relu(self.fc4(out))\n",
    "        out[out>255]=out[out>255]/100\n",
    "        out[out<55]=out[out<55]*1000\n",
    "        '''\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (fc1): Linear(in_features=40, out_features=80, bias=True)\n",
      "  (fc2): Linear(in_features=80, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "gennetwork = Generator().cuda()\n",
    "genoptimizer = optim.Adagrad(gennetwork.parameters(), lr=learning_rate,\n",
    "                            )\n",
    "print(gennetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customloss(inp,bsize):\n",
    "    \n",
    "    inp=torch.cat((inp,100*torch.ones(bsize,1).type(dtype)),1).type(dtype)\n",
    "    cusout=forwardnetwork(inp.reshape([bsize,4])).type(dtype)\n",
    "    \n",
    "    cusout=cusout.reshape([bsize,1]).type(dtype)\n",
    "    cusideal=788*torch.ones([bsize,1]).type(dtype)\n",
    "    \n",
    "    cuscriterion= nn.MSELoss()\n",
    "    \n",
    "    loss= cuscriterion(cusout,cusideal).type(dtype)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgarb=torch.rand([100,40]).type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3986180.2500, device='cuda:0', grad_fn=<AddBackward0>) 0\n",
      "tensor(3975526.5000, device='cuda:0', grad_fn=<AddBackward0>) 1\n",
      "tensor(3964880.5000, device='cuda:0', grad_fn=<AddBackward0>) 2\n",
      "tensor(3954243.7500, device='cuda:0', grad_fn=<AddBackward0>) 3\n",
      "tensor(3943612.2500, device='cuda:0', grad_fn=<AddBackward0>) 4\n",
      "tensor(3932987.2500, device='cuda:0', grad_fn=<AddBackward0>) 5\n",
      "tensor(3922360.5000, device='cuda:0', grad_fn=<AddBackward0>) 6\n",
      "tensor(3911729., device='cuda:0', grad_fn=<AddBackward0>) 7\n",
      "tensor(3901100.2500, device='cuda:0', grad_fn=<AddBackward0>) 8\n",
      "tensor(3890479.2500, device='cuda:0', grad_fn=<AddBackward0>) 9\n",
      "tensor(3879861.7500, device='cuda:0', grad_fn=<AddBackward0>) 10\n",
      "tensor(3869242.7500, device='cuda:0', grad_fn=<AddBackward0>) 11\n",
      "tensor(3858623., device='cuda:0', grad_fn=<AddBackward0>) 12\n",
      "tensor(3848009.7500, device='cuda:0', grad_fn=<AddBackward0>) 13\n",
      "tensor(3837398., device='cuda:0', grad_fn=<AddBackward0>) 14\n",
      "tensor(3826792.2500, device='cuda:0', grad_fn=<AddBackward0>) 15\n",
      "tensor(3816190.2500, device='cuda:0', grad_fn=<AddBackward0>) 16\n",
      "tensor(3805596.5000, device='cuda:0', grad_fn=<AddBackward0>) 17\n",
      "tensor(3795007.5000, device='cuda:0', grad_fn=<AddBackward0>) 18\n",
      "tensor(3784424.2500, device='cuda:0', grad_fn=<AddBackward0>) 19\n",
      "tensor(3773847., device='cuda:0', grad_fn=<AddBackward0>) 20\n",
      "tensor(3763278.7500, device='cuda:0', grad_fn=<AddBackward0>) 21\n",
      "tensor(3752714.7500, device='cuda:0', grad_fn=<AddBackward0>) 22\n",
      "tensor(3742150.5000, device='cuda:0', grad_fn=<AddBackward0>) 23\n",
      "tensor(3731591., device='cuda:0', grad_fn=<AddBackward0>) 24\n",
      "tensor(3721035.2500, device='cuda:0', grad_fn=<AddBackward0>) 25\n",
      "tensor(3710489.2500, device='cuda:0', grad_fn=<AddBackward0>) 26\n",
      "tensor(3699954.7500, device='cuda:0', grad_fn=<AddBackward0>) 27\n",
      "tensor(3689422.7500, device='cuda:0', grad_fn=<AddBackward0>) 28\n",
      "tensor(3678898.5000, device='cuda:0', grad_fn=<AddBackward0>) 29\n",
      "tensor(3668383.5000, device='cuda:0', grad_fn=<AddBackward0>) 30\n",
      "tensor(3657873.7500, device='cuda:0', grad_fn=<AddBackward0>) 31\n",
      "tensor(3647368.5000, device='cuda:0', grad_fn=<AddBackward0>) 32\n",
      "tensor(3636866., device='cuda:0', grad_fn=<AddBackward0>) 33\n",
      "tensor(3626371.7500, device='cuda:0', grad_fn=<AddBackward0>) 34\n",
      "tensor(3615884., device='cuda:0', grad_fn=<AddBackward0>) 35\n",
      "tensor(3605402.2500, device='cuda:0', grad_fn=<AddBackward0>) 36\n",
      "tensor(3594926.5000, device='cuda:0', grad_fn=<AddBackward0>) 37\n",
      "tensor(3584459.2500, device='cuda:0', grad_fn=<AddBackward0>) 38\n",
      "tensor(3574005.5000, device='cuda:0', grad_fn=<AddBackward0>) 39\n",
      "tensor(3563555.7500, device='cuda:0', grad_fn=<AddBackward0>) 40\n",
      "tensor(3553113., device='cuda:0', grad_fn=<AddBackward0>) 41\n",
      "tensor(3542673.7500, device='cuda:0', grad_fn=<AddBackward0>) 42\n",
      "tensor(3532245.2500, device='cuda:0', grad_fn=<AddBackward0>) 43\n",
      "tensor(3521826., device='cuda:0', grad_fn=<AddBackward0>) 44\n",
      "tensor(3511414.5000, device='cuda:0', grad_fn=<AddBackward0>) 45\n",
      "tensor(3501011.2500, device='cuda:0', grad_fn=<AddBackward0>) 46\n",
      "tensor(3490615.7500, device='cuda:0', grad_fn=<AddBackward0>) 47\n",
      "tensor(3480223.5000, device='cuda:0', grad_fn=<AddBackward0>) 48\n",
      "tensor(3469842.7500, device='cuda:0', grad_fn=<AddBackward0>) 49\n",
      "tensor(3459470.2500, device='cuda:0', grad_fn=<AddBackward0>) 50\n",
      "tensor(3449104.5000, device='cuda:0', grad_fn=<AddBackward0>) 51\n",
      "tensor(3438746.2500, device='cuda:0', grad_fn=<AddBackward0>) 52\n",
      "tensor(3428394., device='cuda:0', grad_fn=<AddBackward0>) 53\n",
      "tensor(3418049.2500, device='cuda:0', grad_fn=<AddBackward0>) 54\n",
      "tensor(3407714., device='cuda:0', grad_fn=<AddBackward0>) 55\n",
      "tensor(3397382.2500, device='cuda:0', grad_fn=<AddBackward0>) 56\n",
      "tensor(3387058., device='cuda:0', grad_fn=<AddBackward0>) 57\n",
      "tensor(3376738.5000, device='cuda:0', grad_fn=<AddBackward0>) 58\n",
      "tensor(3366426.5000, device='cuda:0', grad_fn=<AddBackward0>) 59\n",
      "tensor(3356121.2500, device='cuda:0', grad_fn=<AddBackward0>) 60\n",
      "tensor(3345823., device='cuda:0', grad_fn=<AddBackward0>) 61\n",
      "tensor(3335532., device='cuda:0', grad_fn=<AddBackward0>) 62\n",
      "tensor(3325249.7500, device='cuda:0', grad_fn=<AddBackward0>) 63\n",
      "tensor(3314977., device='cuda:0', grad_fn=<AddBackward0>) 64\n",
      "tensor(3304712.7500, device='cuda:0', grad_fn=<AddBackward0>) 65\n",
      "tensor(3294457., device='cuda:0', grad_fn=<AddBackward0>) 66\n",
      "tensor(3284207.5000, device='cuda:0', grad_fn=<AddBackward0>) 67\n",
      "tensor(3273966.7500, device='cuda:0', grad_fn=<AddBackward0>) 68\n",
      "tensor(3263738., device='cuda:0', grad_fn=<AddBackward0>) 69\n",
      "tensor(3253510.2500, device='cuda:0', grad_fn=<AddBackward0>) 70\n",
      "tensor(3243292., device='cuda:0', grad_fn=<AddBackward0>) 71\n",
      "tensor(3233079.5000, device='cuda:0', grad_fn=<AddBackward0>) 72\n",
      "tensor(3222877.7500, device='cuda:0', grad_fn=<AddBackward0>) 73\n",
      "tensor(3212685., device='cuda:0', grad_fn=<AddBackward0>) 74\n",
      "tensor(3202504., device='cuda:0', grad_fn=<AddBackward0>) 75\n",
      "tensor(3192330.2500, device='cuda:0', grad_fn=<AddBackward0>) 76\n",
      "tensor(3182164.2500, device='cuda:0', grad_fn=<AddBackward0>) 77\n",
      "tensor(3172009.2500, device='cuda:0', grad_fn=<AddBackward0>) 78\n",
      "tensor(3161867.2500, device='cuda:0', grad_fn=<AddBackward0>) 79\n",
      "tensor(3151734.5000, device='cuda:0', grad_fn=<AddBackward0>) 80\n",
      "tensor(3141611., device='cuda:0', grad_fn=<AddBackward0>) 81\n",
      "tensor(3131495., device='cuda:0', grad_fn=<AddBackward0>) 82\n",
      "tensor(3121386., device='cuda:0', grad_fn=<AddBackward0>) 83\n",
      "tensor(3111290.5000, device='cuda:0', grad_fn=<AddBackward0>) 84\n",
      "tensor(3101203.7500, device='cuda:0', grad_fn=<AddBackward0>) 85\n",
      "tensor(3091123.2500, device='cuda:0', grad_fn=<AddBackward0>) 86\n",
      "tensor(3081050., device='cuda:0', grad_fn=<AddBackward0>) 87\n",
      "tensor(3070985., device='cuda:0', grad_fn=<AddBackward0>) 88\n",
      "tensor(3060930., device='cuda:0', grad_fn=<AddBackward0>) 89\n",
      "tensor(3050885., device='cuda:0', grad_fn=<AddBackward0>) 90\n",
      "tensor(3040852., device='cuda:0', grad_fn=<AddBackward0>) 91\n",
      "tensor(3030829.7500, device='cuda:0', grad_fn=<AddBackward0>) 92\n",
      "tensor(3020816.2500, device='cuda:0', grad_fn=<AddBackward0>) 93\n",
      "tensor(3010811.7500, device='cuda:0', grad_fn=<AddBackward0>) 94\n",
      "tensor(3000817.2500, device='cuda:0', grad_fn=<AddBackward0>) 95\n",
      "tensor(2990832.2500, device='cuda:0', grad_fn=<AddBackward0>) 96\n",
      "tensor(2980858.5000, device='cuda:0', grad_fn=<AddBackward0>) 97\n",
      "tensor(2970896.7500, device='cuda:0', grad_fn=<AddBackward0>) 98\n",
      "tensor(2960945.7500, device='cuda:0', grad_fn=<AddBackward0>) 99\n",
      "tensor(2951004.7500, device='cuda:0', grad_fn=<AddBackward0>) 100\n",
      "tensor(2941074.7500, device='cuda:0', grad_fn=<AddBackward0>) 101\n",
      "tensor(2931154., device='cuda:0', grad_fn=<AddBackward0>) 102\n",
      "tensor(2921245.7500, device='cuda:0', grad_fn=<AddBackward0>) 103\n",
      "tensor(2911349., device='cuda:0', grad_fn=<AddBackward0>) 104\n",
      "tensor(2901462.5000, device='cuda:0', grad_fn=<AddBackward0>) 105\n",
      "tensor(2891585.7500, device='cuda:0', grad_fn=<AddBackward0>) 106\n",
      "tensor(2881717.7500, device='cuda:0', grad_fn=<AddBackward0>) 107\n",
      "tensor(2871862.2500, device='cuda:0', grad_fn=<AddBackward0>) 108\n",
      "tensor(2862016., device='cuda:0', grad_fn=<AddBackward0>) 109\n",
      "tensor(2852179.2500, device='cuda:0', grad_fn=<AddBackward0>) 110\n",
      "tensor(2842352.5000, device='cuda:0', grad_fn=<AddBackward0>) 111\n",
      "tensor(2832536.2500, device='cuda:0', grad_fn=<AddBackward0>) 112\n",
      "tensor(2822729.5000, device='cuda:0', grad_fn=<AddBackward0>) 113\n",
      "tensor(2812935.5000, device='cuda:0', grad_fn=<AddBackward0>) 114\n",
      "tensor(2803149.7500, device='cuda:0', grad_fn=<AddBackward0>) 115\n",
      "tensor(2793376.7500, device='cuda:0', grad_fn=<AddBackward0>) 116\n",
      "tensor(2783612., device='cuda:0', grad_fn=<AddBackward0>) 117\n",
      "tensor(2773860., device='cuda:0', grad_fn=<AddBackward0>) 118\n",
      "tensor(2764118., device='cuda:0', grad_fn=<AddBackward0>) 119\n",
      "tensor(2754385.7500, device='cuda:0', grad_fn=<AddBackward0>) 120\n",
      "tensor(2744664.7500, device='cuda:0', grad_fn=<AddBackward0>) 121\n",
      "tensor(2734955., device='cuda:0', grad_fn=<AddBackward0>) 122\n",
      "tensor(2725259.7500, device='cuda:0', grad_fn=<AddBackward0>) 123\n",
      "tensor(2715572., device='cuda:0', grad_fn=<AddBackward0>) 124\n",
      "tensor(2705898., device='cuda:0', grad_fn=<AddBackward0>) 125\n",
      "tensor(2696234.2500, device='cuda:0', grad_fn=<AddBackward0>) 126\n",
      "tensor(2686581.5000, device='cuda:0', grad_fn=<AddBackward0>) 127\n",
      "tensor(2676938.7500, device='cuda:0', grad_fn=<AddBackward0>) 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2667307., device='cuda:0', grad_fn=<AddBackward0>) 129\n",
      "tensor(2657687.7500, device='cuda:0', grad_fn=<AddBackward0>) 130\n",
      "tensor(2648079.5000, device='cuda:0', grad_fn=<AddBackward0>) 131\n",
      "tensor(2638480.2500, device='cuda:0', grad_fn=<AddBackward0>) 132\n",
      "tensor(2628892.2500, device='cuda:0', grad_fn=<AddBackward0>) 133\n",
      "tensor(2619315.7500, device='cuda:0', grad_fn=<AddBackward0>) 134\n",
      "tensor(2609750.5000, device='cuda:0', grad_fn=<AddBackward0>) 135\n",
      "tensor(2600198.7500, device='cuda:0', grad_fn=<AddBackward0>) 136\n",
      "tensor(2590657.5000, device='cuda:0', grad_fn=<AddBackward0>) 137\n",
      "tensor(2581128.2500, device='cuda:0', grad_fn=<AddBackward0>) 138\n",
      "tensor(2571613.2500, device='cuda:0', grad_fn=<AddBackward0>) 139\n",
      "tensor(2562110., device='cuda:0', grad_fn=<AddBackward0>) 140\n",
      "tensor(2552617.7500, device='cuda:0', grad_fn=<AddBackward0>) 141\n",
      "tensor(2543136.5000, device='cuda:0', grad_fn=<AddBackward0>) 142\n",
      "tensor(2533668.5000, device='cuda:0', grad_fn=<AddBackward0>) 143\n",
      "tensor(2524211., device='cuda:0', grad_fn=<AddBackward0>) 144\n",
      "tensor(2514768., device='cuda:0', grad_fn=<AddBackward0>) 145\n",
      "tensor(2505341.5000, device='cuda:0', grad_fn=<AddBackward0>) 146\n",
      "tensor(2495927., device='cuda:0', grad_fn=<AddBackward0>) 147\n",
      "tensor(2486524.7500, device='cuda:0', grad_fn=<AddBackward0>) 148\n",
      "tensor(2477136.5000, device='cuda:0', grad_fn=<AddBackward0>) 149\n",
      "tensor(2467761.7500, device='cuda:0', grad_fn=<AddBackward0>) 150\n",
      "tensor(2458401., device='cuda:0', grad_fn=<AddBackward0>) 151\n",
      "tensor(2449050.7500, device='cuda:0', grad_fn=<AddBackward0>) 152\n",
      "tensor(2439712.7500, device='cuda:0', grad_fn=<AddBackward0>) 153\n",
      "tensor(2430390.5000, device='cuda:0', grad_fn=<AddBackward0>) 154\n",
      "tensor(2421079.7500, device='cuda:0', grad_fn=<AddBackward0>) 155\n",
      "tensor(2411784.5000, device='cuda:0', grad_fn=<AddBackward0>) 156\n",
      "tensor(2402502., device='cuda:0', grad_fn=<AddBackward0>) 157\n",
      "tensor(2393235., device='cuda:0', grad_fn=<AddBackward0>) 158\n",
      "tensor(2383975., device='cuda:0', grad_fn=<AddBackward0>) 159\n",
      "tensor(2374726.7500, device='cuda:0', grad_fn=<AddBackward0>) 160\n",
      "tensor(2365491.7500, device='cuda:0', grad_fn=<AddBackward0>) 161\n",
      "tensor(2356268.2500, device='cuda:0', grad_fn=<AddBackward0>) 162\n",
      "tensor(2347058.7500, device='cuda:0', grad_fn=<AddBackward0>) 163\n",
      "tensor(2337858.5000, device='cuda:0', grad_fn=<AddBackward0>) 164\n",
      "tensor(2328669.7500, device='cuda:0', grad_fn=<AddBackward0>) 165\n",
      "tensor(2319488.7500, device='cuda:0', grad_fn=<AddBackward0>) 166\n",
      "tensor(2310318.5000, device='cuda:0', grad_fn=<AddBackward0>) 167\n",
      "tensor(2301159.2500, device='cuda:0', grad_fn=<AddBackward0>) 168\n",
      "tensor(2292012.2500, device='cuda:0', grad_fn=<AddBackward0>) 169\n",
      "tensor(2282877.5000, device='cuda:0', grad_fn=<AddBackward0>) 170\n",
      "tensor(2273754., device='cuda:0', grad_fn=<AddBackward0>) 171\n",
      "tensor(2264644., device='cuda:0', grad_fn=<AddBackward0>) 172\n",
      "tensor(2255548.7500, device='cuda:0', grad_fn=<AddBackward0>) 173\n",
      "tensor(2246466., device='cuda:0', grad_fn=<AddBackward0>) 174\n",
      "tensor(2237397.7500, device='cuda:0', grad_fn=<AddBackward0>) 175\n",
      "tensor(2228343.2500, device='cuda:0', grad_fn=<AddBackward0>) 176\n",
      "tensor(2219305., device='cuda:0', grad_fn=<AddBackward0>) 177\n",
      "tensor(2210281., device='cuda:0', grad_fn=<AddBackward0>) 178\n",
      "tensor(2201272.7500, device='cuda:0', grad_fn=<AddBackward0>) 179\n",
      "tensor(2192278.7500, device='cuda:0', grad_fn=<AddBackward0>) 180\n",
      "tensor(2183298.5000, device='cuda:0', grad_fn=<AddBackward0>) 181\n",
      "tensor(2174332.2500, device='cuda:0', grad_fn=<AddBackward0>) 182\n",
      "tensor(2165381.2500, device='cuda:0', grad_fn=<AddBackward0>) 183\n",
      "tensor(2156443.2500, device='cuda:0', grad_fn=<AddBackward0>) 184\n",
      "tensor(2147516., device='cuda:0', grad_fn=<AddBackward0>) 185\n",
      "tensor(2138604.5000, device='cuda:0', grad_fn=<AddBackward0>) 186\n",
      "tensor(2129703.7500, device='cuda:0', grad_fn=<AddBackward0>) 187\n",
      "tensor(2120816.7500, device='cuda:0', grad_fn=<AddBackward0>) 188\n",
      "tensor(2111944., device='cuda:0', grad_fn=<AddBackward0>) 189\n",
      "tensor(2103084., device='cuda:0', grad_fn=<AddBackward0>) 190\n",
      "tensor(2094240.1250, device='cuda:0', grad_fn=<AddBackward0>) 191\n",
      "tensor(2085407.3750, device='cuda:0', grad_fn=<AddBackward0>) 192\n",
      "tensor(2076587.5000, device='cuda:0', grad_fn=<AddBackward0>) 193\n",
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "batchsize=2\n",
    "\n",
    "batches=len(xgarb)/batchsize\n",
    "\n",
    "epochs=1000\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    epochloss=0\n",
    "        \n",
    "    for j in range(int(batches)):\n",
    "        \n",
    "        \n",
    "        #print(Ytensor)\n",
    "        out=gennetwork(xgarb[j*batchsize:(j+1)*batchsize].reshape([batchsize,40])).type(dtype)\n",
    "        \n",
    "        if out[0][0]>100:\n",
    "            print('done')\n",
    "            break\n",
    "        #compute loss\n",
    "        loss = customloss(out,batchsize).type(dtype)\n",
    "\n",
    "\n",
    "        #backprop loss i.e. find dloss/dparam for each parameter and store.\n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        #clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(gennetwork.parameters(), 1.0)\n",
    "        genoptimizer.step()\n",
    "        \n",
    "        epochloss=epochloss+loss\n",
    "    \n",
    "    if out[0][0]>100:\n",
    "            print('done')\n",
    "            break\n",
    "    print(epochloss,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(47441.8828, device='cuda:0', grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[100.0423,   0.0000,  53.7249],\n",
      "        [ 81.6447,   0.0000,  43.8124]], device='cuda:0',\n",
      "       grad_fn=<IndexPutBackward>)\n",
      "tensor([777.5806], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out)\n",
    "print(output(out[0][0],out[0][1],out[0][2],100*torch.ones(1).type(dtype)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([787.3434], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output(100,100,100,100*torch.ones(1).type(dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xtens0=torch.linspace(0,10,1000).type(dtype)\n",
    "xtens1=torch.linspace(2,54,1000).type(dtype)\n",
    "xtens2=torch.linspace(1,100,1000).type(dtype)\n",
    "xtens3=torch.linspace(10,100,1000).type(dtype)\n",
    "\n",
    "ytens=torch.empty(1000,1).type(dtype)\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    ytens[i]=out(xtens0[i],xtens1[i],xtens2[i],xtens3[i]).type(dtype)\n",
    "\n",
    "print(ytens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
